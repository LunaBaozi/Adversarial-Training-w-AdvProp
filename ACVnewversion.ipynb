{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6K3yyhQaUE0"
      },
      "source": [
        "# ADVERSARIAL EXAMPLES IMPROVE IMAGE RECOGNITION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAP5qf1XabOP"
      },
      "source": [
        "### Importing Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuZ0Aq-yN4Tn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsBa3BZXafeb"
      },
      "source": [
        "### Installing the Foolbox library\n",
        "#### It will be used to generate adversarial attacks and to attack the networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "46iGdy07K2It"
      },
      "outputs": [],
      "source": [
        "!pip install foolbox==3.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt_7zGvbapo7"
      },
      "source": [
        "### Importing the needed libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDxbBQooNvAj"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import foolbox as fb\n",
        "\n",
        "from math import ceil\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ1vUEk0MNUD"
      },
      "source": [
        "### Setting a manual seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2TA9S-sMMlg"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LKOWFpnatO8"
      },
      "source": [
        "### Defining the needed data transformations and creating the DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYHUyBZWLmva"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, batch_size, test_batch_size=256): \n",
        "  transform = list()\n",
        "\n",
        "  if dataset == 'MNIST':\n",
        "    transform.append(torchvision.transforms.ToTensor()) \n",
        "    transform.append(torchvision.transforms.Lambda(lambda x: x.repeat(3,1,1)))\n",
        "    transform.append(torchvision.transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]))\n",
        "    transform = torchvision.transforms.Compose(transform)\n",
        "    full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True) \n",
        "    test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  \n",
        "  elif dataset == 'SVHN':\n",
        "    transform.append(torchvision.transforms.ToTensor()) \n",
        "    transform.append(torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]))\n",
        "    transform = torchvision.transforms.Compose(transform)\n",
        "    full_training_data = torchvision.datasets.SVHN('./data', split='train', transform=transform, download=True) \n",
        "    test_data = torchvision.datasets.SVHN('./data', split='test', transform=transform, download=True) \n",
        "\n",
        "  elif dataset == 'CIFAR':\n",
        "    transform.append(torchvision.transforms.ToTensor()) \n",
        "    #transform.append(torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.247, 0.243, 0.261]))\n",
        "    transform.append(torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]))\n",
        "    transform = torchvision.transforms.Compose(transform)\n",
        "    full_training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True) \n",
        "    test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True) \n",
        "  \n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False) \n",
        "  \n",
        "  return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3rtBsyan7R4"
      },
      "source": [
        "### Defining the scaling parameters for EfficientNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDH0E7YPLp1Y"
      },
      "outputs": [],
      "source": [
        "# SCALING PARAMETERS FOR EFFICIENTNET\n",
        "\n",
        "base_model = [\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\n",
        "    [1, 16, 1, 1, 3],\n",
        "    [6, 24, 2, 2, 3],\n",
        "    [6, 40, 2, 2, 5],\n",
        "    [6, 80, 3, 2, 3],\n",
        "    [6, 112, 3, 1, 5],\n",
        "    [6, 192, 4, 2, 5],\n",
        "    [6, 320, 1, 1, 3],\n",
        "]\n",
        "\n",
        "phi_values = {\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\n",
        "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
        "    \"b1\": (0.5, 240, 0.2),\n",
        "    \"b2\": (1, 260, 0.3),\n",
        "    \"b3\": (2, 300, 0.3),\n",
        "    \"b4\": (3, 380, 0.4),\n",
        "    \"b5\": (4, 456, 0.4),\n",
        "    \"b6\": (5, 528, 0.5),\n",
        "    \"b7\": (6, 600, 0.5),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FXOakbdoAw1"
      },
      "source": [
        "### Defining the CNN, Squeeze-excitation and inverted-residual blocks for the EfficientNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdDJfE_QLp-F"
      },
      "outputs": [],
      "source": [
        "# DEFINITION OF CNN, SQUEEZE-EXCITATION AND INVERTED-RESIDUAL BLOCKS\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, kernel_size, stride, padding, mode, groups=1,\n",
        "    ):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.cnn = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            groups=groups,\n",
        "            bias = False,\n",
        "        )\n",
        "        self.mode = mode\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        if self.mode == 'advprop':\n",
        "          self.bn_adv = nn.BatchNorm2d(out_channels)\n",
        "        if self.mode == 'advprop2':\n",
        "          self.bn_adv = nn.BatchNorm2d(out_channels)\n",
        "          self.bn_adv2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.silu = nn.SiLU() \n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.training:\n",
        "        x = self.cnn(x)\n",
        "\n",
        "        if self.mode == 'vanilla':\n",
        "          x = self.silu(self.bn(x))\n",
        "    \n",
        "        elif self.mode == 'advprop':\n",
        "          x_clean, x_adv = torch.split(x, split_size_or_sections = x.shape[0] // 2, dim = 0)\n",
        "          x_clean = self.bn(x_clean)\n",
        "          x_adv = self.bn_adv(x_adv)\n",
        "          x = torch.cat((x_clean, x_adv), dim = 0)\n",
        "          x = self.silu(x)\n",
        "        \n",
        "        elif self.mode == 'advprop2':\n",
        "          x_clean, x_adv, x_adv2 = torch.split(x,split_size_or_sections=x.shape[0] // 3, dim = 0)\n",
        "          x_clean = self.bn(x_clean)\n",
        "          x_adv = self.bn_adv(x_adv)\n",
        "          x_adv2 = self.bn_adv2(x_adv2)\n",
        "          x = torch.cat((x_clean, x_adv, x_adv2), dim = 0)\n",
        "  \n",
        "      else:\n",
        "        x=self.silu(self.bn(self.cnn(x)))\n",
        "      \n",
        "      return x\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_channels, reduced_dim):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
        "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            mode,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            expand_ratio,\n",
        "            reduction = 4, \n",
        "            survival_prob = 0.8\n",
        "            ):\n",
        "      \n",
        "        super(InvertedResidualBlock, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        if self.mode == 'vanilla':\n",
        "          self.bn = nn.BatchNorm2d(out_channels)\n",
        "        elif self.mode == 'advprop':\n",
        "          self.bn = nn.BatchNorm2d(out_channels)\n",
        "          self.bn_adv = nn.BatchNorm2d(out_channels)\n",
        "        elif self.mode == 'advprop2':\n",
        "          self.bn = nn.BatchNorm2d(out_channels)\n",
        "          self.bn_adv = nn.BatchNorm2d(out_channels)\n",
        "          self.bn_adv2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.survival_prob = 0.8\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.expand = in_channels != hidden_dim\n",
        "        reduced_dim = int(in_channels / reduction)\n",
        "\n",
        "        if self.expand:\n",
        "            self.expand_conv = CNNBlock(in_channels, \n",
        "                                        hidden_dim, \n",
        "                                        kernel_size = 3, \n",
        "                                        stride = 1, \n",
        "                                        padding = 1,\n",
        "                                        mode = self.mode\n",
        "                                        )\n",
        "\n",
        "        self.conv = nn.Sequential(CNNBlock(hidden_dim, \n",
        "                                           hidden_dim, \n",
        "                                           kernel_size, \n",
        "                                           stride, \n",
        "                                           padding, \n",
        "                                           mode = self.mode,\n",
        "                                           groups = hidden_dim),\n",
        "                                  SqueezeExcitation(hidden_dim, reduced_dim),\n",
        "                                  nn.Conv2d(hidden_dim, out_channels, 1, bias = False)\n",
        "                                  )\n",
        "\n",
        "    def stochastic_depth(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "\n",
        "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device = x.device) < self.survival_prob\n",
        "        \n",
        "        return torch.div(x, self.survival_prob) * binary_tensor\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.expand_conv(inputs) if self.expand else inputs\n",
        "        x = self.conv(x)\n",
        "\n",
        "        if self.training:\n",
        "          if self.mode == 'vanilla':\n",
        "            x = self.bn(x)\n",
        "          elif self.mode == 'advprop':\n",
        "            x_clean, x_adv = torch.split(x, split_size_or_sections = x.shape[0] // 2, dim = 0) \n",
        "            x_clean = self.bn(x_clean)\n",
        "            x_adv = self.bn_adv(x_adv)\n",
        "            x = torch.cat((x_clean, x_adv), dim = 0)\n",
        "          elif self.mode == 'advprop2':\n",
        "            x_clean, x_adv, x_adv2 = torch.split(x, split_size_or_sections = x.shape[0] // 3, dim = 0)\n",
        "            x_clean = self.bn(x_clean)\n",
        "            x_adv = self.bn_adv(x_adv)\n",
        "            x_adv2 = self.bn_adv2(x_adv2)\n",
        "            x = torch.cat((x_clean, x_adv, x_adv2), dim = 0)\n",
        "           \n",
        "        else:\n",
        "          x = self.bn(x)\n",
        "\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(x) + inputs\n",
        "        else:\n",
        "            return x #self.conv(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idg_PwB4oIT3"
      },
      "source": [
        "### Defining the EfficientNet class for building the whole architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTYsX16dLuo7"
      },
      "outputs": [],
      "source": [
        "# DEFINITION OF THE ARCHITECTURE\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, version, num_classes, mode):   \n",
        "        super(EfficientNet, self).__init__()\n",
        "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
        "        last_channels = ceil(1280 * width_factor)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.mode = mode\n",
        "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channels, num_classes),\n",
        "        )\n",
        "\n",
        "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
        "        phi, res, drop_rate = phi_values[version]\n",
        "        depth_factor = alpha ** phi\n",
        "        width_factor = beta ** phi\n",
        "        return width_factor, depth_factor, drop_rate\n",
        "\n",
        "    def create_features(self, width_factor, depth_factor, last_channels):\n",
        "        channels = int(32 * width_factor)\n",
        "        features = [CNNBlock(3, channels, 3, stride=2, padding=1, mode = self.mode)]\n",
        "        in_channels = channels\n",
        "\n",
        "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
        "            out_channels = 4*ceil(int(channels*width_factor) / 4)\n",
        "            layers_repeats = ceil(repeats * depth_factor)\n",
        "\n",
        "            for layer in range(layers_repeats):\n",
        "                features.append(\n",
        "                    InvertedResidualBlock(\n",
        "                        self.mode,\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        expand_ratio=expand_ratio,\n",
        "                        stride = stride if layer == 0 else 1,\n",
        "                        kernel_size=kernel_size,\n",
        "                        padding=kernel_size//2, \n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "        features.append(\n",
        "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0, mode = self.mode)\n",
        "        )\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.features(x))\n",
        "        return self.classifier(x.view(x.shape[0], -1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_3QvhKMoN03"
      },
      "source": [
        "### Defining the class for early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0G236fOLuxu"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping():\n",
        "    def __init__(self, min_delta = 0, patience = 0):\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = np.Inf\n",
        "        self.stop_training = False\n",
        "    def on_epoch_end(self, epoch, current_value):\n",
        "        if np.greater(self.best, (current_value - self.min_delta)):\n",
        "            self.best = current_value\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait > self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.stop_training = True\n",
        "        return self.stop_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw5V00uDoRol"
      },
      "source": [
        "### Definining the cross entropy cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU4Ek6TlLxk3"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v5QW8s3ob7R"
      },
      "source": [
        "### Defining the stochastic gradient descent optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3upaolGJLxoI"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer\n",
        "\n",
        "# def get_optimizer(net, lr, wd, momentum):\n",
        "#   optimizer = torch.optim.RMSprop(net.parameters(), lr = lr, weight_decay = wd, momentum = momentum)\n",
        "#   return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eczh8XYAWAuG"
      },
      "source": [
        "### Defining the learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTZ2vib9V_zQ"
      },
      "outputs": [],
      "source": [
        "# def get_scheduler(optimizer, step_size, gamma):\n",
        "#   scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)\n",
        "#   return scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aacAlKu5ogKL"
      },
      "source": [
        "### Defining the train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1QpHzbdLxrr"
      },
      "outputs": [],
      "source": [
        "def test(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() \n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() \n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(net, mode, data_loader, optimizer, cost_function, device='cuda'):   #scheduler\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() \n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    if mode == 'advprop':\n",
        "      attack = fb.attacks.PGD(rel_stepsize = 1, steps = 1)\n",
        "      raw, clipped, is_adv = attack(foolbox_model_vanilla, inputs, targets, epsilons=0.003)\n",
        "      raw = raw.to(device)\n",
        "      inputs = torch.cat((inputs, raw), dim = 0) \n",
        "\n",
        "    if mode == 'advprop2':\n",
        "      attack = fb.attacks.PGD(rel_stepsize = 1, steps = 1)\n",
        "      attack2 = fb.attacks.VirtualAdversarialAttack(steps = 1, xi = 0.13)\n",
        "      raw, clipped, is_adv = attack(foolbox_model_vanilla, inputs, targets, epsilons=0.0157)\n",
        "      raw2, clipped2, is_adv2 = attack2(foolbox_model_vanilla, inputs, targets, epsilons=0.26)\n",
        "      raw = raw.to(device)\n",
        "      raw2 = raw2.to(device)\n",
        "      inputs=torch.cat((inputs, raw, raw2),dim=0)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    if mode == 'vanilla':\n",
        "      loss = cost_function(outputs, targets)\n",
        "    elif mode == 'advprop':\n",
        "      out1, out2 = torch.split(outputs, split_size_or_sections = outputs.shape[0] // 2, dim = 0)\n",
        "      loss1 = cost_function(out1, targets)\n",
        "      loss2 = cost_function(out2, targets)\n",
        "      loss = loss1 + loss2\n",
        "      targets = torch.cat((targets, targets), dim = 0)\n",
        "    elif mode == 'advprop2':\n",
        "      out1, out2, out3 = torch.split(outputs, split_size_or_sections = outputs.shape[0] // 3, dim = 0)\n",
        "      loss1 = cost_function(out1, targets)\n",
        "      loss2 = cost_function(out2, targets)\n",
        "      loss3 = cost_function(out3, targets)\n",
        "      loss = loss1 + loss2 + loss3\n",
        "      targets = torch.cat((targets, targets, targets),dim = 0) \n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    # Decaying the learning rate\n",
        "    # scheduler.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) \n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2oDnzR3oxPT"
      },
      "source": [
        "### Wrapping all up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ik3eknLu4n"
      },
      "outputs": [],
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
        "\n",
        "def main(dataset,\n",
        "         mode,\n",
        "         batch_size = 128, \n",
        "         #input_dim=28*28, \n",
        "         #hidden_dim=100, \n",
        "         #output_dim=10, \n",
        "         device = 'cuda:0', \n",
        "         learning_rate = 0.01,  #0.256 \n",
        "         weight_decay = 0.000001,  #0.9\n",
        "         momentum = 0.9, \n",
        "         epochs = 50\n",
        "         ):\n",
        "  \n",
        "  from torch.utils.tensorboard import SummaryWriter\n",
        "  # Creates a logger for the experiment\n",
        "  writer = SummaryWriter(log_dir = \"runs/exp1\")\n",
        "\n",
        "  # Gets DataLoaders\n",
        "  train_loader, val_loader, test_loader = get_data(dataset, batch_size)\n",
        "  \n",
        "  # Defining needed parameters\n",
        "  version = 'b0'\n",
        "  num_classes = 10\n",
        "\n",
        "  net = EfficientNet(version = version, \n",
        "                     num_classes = num_classes,\n",
        "                     mode = mode).to(device)   \n",
        "  \n",
        "  # Instantiates the optimizer\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  # Instantiates the learning rate scheduler\n",
        "  # scheduler = get_scheduler(optimizer, step_size = 2.4, gamma=0.001)\n",
        "  \n",
        "  # Creates the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # Computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  # Logs to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"Test\")\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  all_early_stopping = EarlyStopping(patience = 4)\n",
        "  all_early_stopping.stop_training = False\n",
        "  # For each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "    if all_early_stopping.stop_training: \n",
        "      break\n",
        "      print(\"Reached Early Stopping Patience at Epoch {}\".format(e))\n",
        "\n",
        "    train_loss, train_accuracy = train(net, mode, train_loader, optimizer, cost_function)   #scheduler\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "\n",
        "    all_early_stopping.on_epoch_end(epoch = (e + 1), current_value = round(val_loss,5))\n",
        "    if all_early_stopping.wait == 0: \n",
        "      bestModel = net\n",
        "      # current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "      torch.save(bestModel, \"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0\"+mode+dataset+\".pt\")\n",
        "      \n",
        "\n",
        "\n",
        "    # Logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # Compute final evaluation results\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  # Logs to TensorBoard\n",
        "  log_values(writer, epochs, train_loss, train_accuracy, \"Train\")\n",
        "  log_values(writer, epochs, val_loss, val_accuracy, \"Validation\")\n",
        "  log_values(writer, epochs, test_loss, test_accuracy, \"Test\")\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # Closes the logger\n",
        "  writer.close()\n",
        "  return net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seUBRUKnL4BH"
      },
      "outputs": [],
      "source": [
        "! rm -r runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxp1s7BpXpCY"
      },
      "outputs": [],
      "source": [
        "B0_vanilla_SVHN = main(mode = 'vanilla', dataset = 'SVHN')\n",
        "B0_advprop_SVHN = main(mode = 'advprop', dataset = 'SVHN')\n",
        "B0_advprop2_SVHN = main(mode = 'advprop2', dataset = 'SVHN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL_mdkX8duhG"
      },
      "outputs": [],
      "source": [
        "B0_vanilla_CIFAR = main(mode = 'vanilla', dataset = 'CIFAR')\n",
        "B0_advprop_CIFAR = main(mode = 'advprop', dataset = 'CIFAR')\n",
        "B0_advprop2_CIFAR = main(mode = 'advprop2', dataset = 'CIFAR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_CA8BIWfZk-"
      },
      "source": [
        "### Testing robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frdgBQL8EP7R"
      },
      "outputs": [],
      "source": [
        "def get_data_evaluation(test_batch_size=256): \n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = list()\n",
        "  transform.append(torchvision.transforms.ToTensor()) \n",
        "  #transform.append(torchvision.transforms.Lambda(lambda x: x.repeat(3,1,1)))\n",
        "  transform.append(torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))\n",
        "  transform = torchvision.transforms.Compose(transform)\n",
        "\n",
        "  # Load data\n",
        "  #test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  test_data = torchvision.datasets.SVHN('./data', split='test', transform=transform, download=True) \n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False) \n",
        "  return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57mAkesXER_h"
      },
      "outputs": [],
      "source": [
        "def get_mnist(test_batch_size=256): \n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = list()\n",
        "  transform.append(torchvision.transforms.ToTensor()) \n",
        "  transform.append(torchvision.transforms.Lambda(lambda x: x.repeat(3,1,1)))\n",
        "  transform.append(torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))\n",
        "  transform = torchvision.transforms.Compose(transform)\n",
        "\n",
        "  # Load data\n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  #test_data = torchvision.datasets.SVHN('./data', split='test', transform=transform, download=True) \n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False) \n",
        "  return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zDUVPFxi5fZ"
      },
      "outputs": [],
      "source": [
        "def get_CIFAR(test_batch_size=256): \n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = list()\n",
        "  transform.append(torchvision.transforms.ToTensor()) \n",
        "  #transform.append(torchvision.transforms.Lambda(lambda x: x.repeat(3,1,1)))\n",
        "  transform.append(torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]))\n",
        "  transform = torchvision.transforms.Compose(transform)\n",
        "\n",
        "  # Load data\n",
        "  test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True) \n",
        "  #test_data = torchvision.datasets.SVHN('./data', split='test', transform=transform, download=True) \n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False) \n",
        "  return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C60o06rkETt1"
      },
      "outputs": [],
      "source": [
        "# mnist_test_data = get_mnist() \n",
        "cifar_testdata = get_CIFAR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS7a8qcjEVUk"
      },
      "outputs": [],
      "source": [
        "SVHN_valdata = get_data_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdiOtBeiEVXR"
      },
      "outputs": [],
      "source": [
        "B0_vanilla_SVHN = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0vanillaSVHN.pt\")\n",
        "B0_advprop_SVHN = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0advpropSVHN.pt\")\n",
        "B0_advprop2_SVHN = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0advprop2SVHN.pt\")\n",
        "\n",
        "B0_vanilla_CIFAR = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0vanillaCIFAR.pt\")  # normalization 0.5\n",
        "B0_advprop_CIFAR = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0advpropCIFAR.pt\")  # normalization 0.5\n",
        "B0_advprop2_CIFAR = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/ACV_data/ENetB0advpropCIFAR.pt\")  # normalization 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt5ZMcGjEVZ-"
      },
      "outputs": [],
      "source": [
        "bounds = (-1, 1)\n",
        "foolbox_model_vanilla = fb.PyTorchModel(B0_vanilla_CIFAR.eval(), bounds = bounds)\n",
        "foolbox_model_advprop = fb.PyTorchModel(B0_advprop_CIFAR.eval(), bounds = bounds)\n",
        "foolbox_model_advprop2 = fb.PyTorchModel(B0_advprop2_CIFAR.eval(), bounds = bounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOBsGJd3EVcA"
      },
      "outputs": [],
      "source": [
        "def test_robustness(net,net_att, data_loader, device='cuda'):\n",
        "  samples = 0.\n",
        "  #cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  # mean_accuracy=[]\n",
        "\n",
        "  net.eval() \n",
        "  \n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # attack = fb.attacks.PGD(rel_stepsize = 1, steps = 1)\n",
        "      # raw, clipped, is_adv = attack(net_att, inputs, targets, epsilons=0.0078)\n",
        "      attack = fb.attacks.VirtualAdversarialAttack(steps=1, xi=0.01)\n",
        "      raw, clipped, is_adv = attack(net_att, inputs, targets, epsilons=0.6)\n",
        "      outputs = net(raw)\n",
        "\n",
        "      samples+=inputs.shape[0]\n",
        "      #cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG13TXbMEaWT"
      },
      "outputs": [],
      "source": [
        "print(test_robustness(B0_advprop_CIFAR, foolbox_model, cifar_testdata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHAz1a69EaY7"
      },
      "outputs": [],
      "source": [
        "def test_vanilla(net, data_loader, device='cuda'):\n",
        "  samples = 0.\n",
        "  #cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() \n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      outputs = net(inputs)\n",
        "    \n",
        "\n",
        "      # Apply the loss\n",
        "      #loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples+=inputs.shape[0]\n",
        "      #cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQkg9GJnEarJ"
      },
      "outputs": [],
      "source": [
        "# test_SVHN(B0_vanilla_SVHN, SVHN_valdata)\n",
        "test_vanilla(B0_advprop_CIFAR, cifar_testdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoFSLldDEat5"
      },
      "outputs": [],
      "source": [
        "test_vanilla(B0_vanilla_SVHN, mnist_test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ACVnewversion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}